{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "actividad_demanda_de_bicicletas_con_NNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK9dmG7MSOya"
      },
      "source": [
        "#  <center> Taller  de Aprendizaje Automático </center>\n",
        "##  <center> Taller 5: Estimación de la demanda de bicicletas compartidas utilizando *Neural Networks*.  </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PjU4ItOTaGr"
      },
      "source": [
        "#Introducción\n",
        "\n",
        "En esta actividad se retomará el problema de la competencia [*Bike Sharing Demand*](https://www.kaggle.com/c/bike-sharing-demand) visto en el Taller 3.\n",
        "Esta vez las estimaciónes deben obtenerse utilizando la herramienta: *Multilayer Perceptron* (MLP). Es importante mantener la función *Root Mean Squared Logarithmic Error* (RMSLE) como medida de desempeño de manera de poder comparar los resultados con los obtenidos en el Taller 3.\n",
        "\n",
        "Tanto las preguntas teóricas como la parte práctica de esta actividad están ligadas al contenido del capítulo 10 (*Introduction to\n",
        "Artificial Neural Networks with\n",
        "Keras*) del libro del curso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnXnqw5KS3zd"
      },
      "source": [
        "## Objetivos\n",
        "\n",
        "\n",
        "*   Trabajar con modelos MLP utilizando la librería [*Keras*](https://keras.io/api/).\n",
        "*   Probar algunas de las herramientas disponibles para la busqueda de hiperparámetros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "common-destiny"
      },
      "source": [
        "## Formas de trabajo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moral-gallery"
      },
      "source": [
        "### Opción 1: Trabajar localmente\n",
        "\n",
        "Descargar los datos en su máquina personal y trabajar en su propio ambiente de desarrollo. Asumiendo que ya creo un entorno para los talleres anteriores sólo debería installar la librería faltantes.  \n",
        " \n",
        "*conda activate TAA-py38*    \n",
        "*pip install tensorflow keras optuna*          \n",
        "*jupyter-notebook*    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fantastic-happiness"
      },
      "source": [
        "Los paquetes faltantes se pueden instalar desde el notebook haciendo:     \n",
        "*!pip install paquete_faltante*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lined-sport"
      },
      "source": [
        "### Opción 2:  Trabajar en *Colab*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lined-candle"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/TAA-fing/TAA-2021/blob/main/taller2_criticas_cine.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Ejecutar en Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expensive-jewel"
      },
      "source": [
        "Se puede trabajar en Google Colab. Para ello es necesario contar con una cuenta de **google drive** y ejecutar un notebook almacenado en dicha cuenta. De lo contrario, no se conservarán los cambios realizados en la sesión. En caso de ya contar con una cuenta, se puede abrir el notebook y luego ir a *Archivo-->Guardar una copia en drive*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "committed-quarterly"
      },
      "source": [
        "En caso de estar trabajando desde un notebook en Colab, deberá:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "considered-dispatch"
      },
      "source": [
        "a) Installar el paquete *kaggle* para acceder a los datos "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sunrise-major"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahead-austria"
      },
      "source": [
        "b) realizar la configuración necesaria para obtener datos desde la plataforma Kaggle. Para ello deberá ir a la página de la competencia y en la sección *data* aceptar los términos. Luego ejecutar la siguiente celda y pasarle el *token* de su usuario (ver comentario en celda)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGyCgWE6Y2hh"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import files\n",
        "\n",
        "# El siguiente archivo solicitado es para habilitar la API de Kaggle en el entorno que está trabajando.\n",
        "# Este archivo se descarga entrando a su perfíl de Kaggle, en la sección API, presionando donde dice: Create New API Token\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "#Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stretch-sunglasses"
      },
      "source": [
        "Una vez guardado el *token* se pueden descargar los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKYP0IiCZA9G"
      },
      "source": [
        "# Descarga de datos\n",
        "!kaggle competitions download -c bike-sharing-demand\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df_test = pd.read_csv('test.csv')\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_submission = pd.read_csv('sampleSubmission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQi0j-avej0Q"
      },
      "source": [
        "# Datos\n",
        "\n",
        "Dado que ya se ha familiarizado con los datos, implemente el mismo preprocesamiento que utilizó en el Taller 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyhdy7JagA0k"
      },
      "source": [
        "# Multilayer Perceptron (MLP)\n",
        "\n",
        "Siguiendo el ejemplo de la sección *Building a Regression MLP Using the Sequential API*:\n",
        "\n",
        "\n",
        "*   Implementar un estimador manteniendo los hiperparámetros del ejemplo.\n",
        "*   ¿Cuál es la cantidad total de parámetros entrenables de la red?\n",
        "*   Seleccionar aleatoriamente un *10%* de los datos para validación, y graficar la función de *loss* (*Mean Squared Logarithmic Error*) de entrenamiento y validación.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzchTK1jrIxA"
      },
      "source": [
        "# Ajuste fino (Scikit-Learn)\n",
        "\n",
        "Siguiendo el ejemplo de la sección *Fine-Tuning Neural Networks Hyperparameters*.\n",
        "\n",
        "\n",
        "\n",
        "*   Utilizar la herramienta *RandomizedSearchCV* de *Scikit-Learn* para la busqueda de hiperparámetros del modelo implementado en *keras*. \n",
        "*   Probar el *tip* que se sugiere en la sección *Number of Neurons per Hidden Layer* y comentar los resultados.\n",
        "\n",
        "\n",
        "Nota: Debido a los recientes cambios de *Scikit-Learn* la ejecución del codigo del ejemplo puede devolver errores. Revisar la sección *Hyperparameter Tuning* en el [notebook](https://github.com/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb) del capítulo, en la cual se resuelven estos errores.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmUt2QrywG60"
      },
      "source": [
        "# Ajuste fino (Optuna)\n",
        "\n",
        "*   Utilizar *Optuna* para la busqueda de hiperparámetros del modelo en *Keras*. Se le sugiere seguir uno de los siguientes ejemplos: [*keras_simple*](https://github.com/optuna/optuna/blob/master/examples/keras/keras_simple.py),  [OptunaSearchCV](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.OptunaSearchCV.html#optuna.integration.OptunaSearchCV)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQoU9uS1lkM0"
      },
      "source": [
        "# Pipeline\n",
        "\n",
        "\n",
        "\n",
        "*   Incorporar el estimador con mejor desempeño a un *pipeline* similar al implementado en el taller 3.\n",
        "*   Subir los resultados de los datos *test* a la página de la competencia.\n",
        "\n"
      ]
    }
  ]
}