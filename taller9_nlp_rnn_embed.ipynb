{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taller9_nlp_rnn_embed.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjsf78ymkS0E"
      },
      "source": [
        "#  <center> Taller  de Aprendizaje Automático </center>\n",
        "##  <center> Taller 9: *Natural Language Processing* (NLP)  </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4y3HbLOkOVD"
      },
      "source": [
        "#Introducción\n",
        "La siguiente actividad propone el abordaje de un problema de procesamiento de lenguaje natural (NLP) utilizando herramientas de *embedding* y modelos RNN. El conjunto de datos que se utilizará es IMDb, el cual corresponde a un problema de clasificación donde se tienen 50000 criticas de peliculas (25000 de *train* y 25000 de *test*), y se quiere estimar si estas son críticas positiva (1) o negativa (0). \n",
        "\n",
        "El trabajo a realizar básicamente consiste en entender y reproducir los pasos de la sección *Sentiment Analysis* para los datos **sin procesar**, agregando algunas variantes como mitigar el sobreajuste y entender la herramienta *embeddings*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY6vz2Ekj8ig"
      },
      "source": [
        "## Objetivos\n",
        "\n",
        "\n",
        "*   Aplicar modelos basados en RNN a un problema de NLP.\n",
        "*   Trabajar con embeddings para secuencias de texto, en particular embeddings preentrenados.\n",
        "*   Utilizar herramientas para la visualización de embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "common-destiny"
      },
      "source": [
        "## Formas de trabajo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moral-gallery"
      },
      "source": [
        "### Opción 1: Trabajar localmente\n",
        "\n",
        "Descargar los datos en su máquina personal y trabajar en su propio ambiente de desarrollo. Asumiendo que ya creo un entorno para los talleres anteriores sólo debería installar la librería faltantes.  \n",
        " \n",
        "*conda activate TAA-py38*    \n",
        "*pip install xgboost seaborn*          \n",
        "*jupyter-notebook*    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fantastic-happiness"
      },
      "source": [
        "Los paquetes faltantes se pueden instalar desde el notebook haciendo:     \n",
        "*!pip install paquete_faltante*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lined-sport"
      },
      "source": [
        "### Opción 2:  Trabajar en *Colab*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lined-candle"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/TAA-fing/TAA-2021/blob/main/taller9_nlp_rnn_embed.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Ejecutar en Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expensive-jewel"
      },
      "source": [
        "Se puede trabajar en Google Colab. Para ello es necesario contar con una cuenta de **google drive** y ejecutar un notebook almacenado en dicha cuenta. De lo contrario, no se conservarán los cambios realizados en la sesión. En caso de ya contar con una cuenta, se puede abrir el notebook y luego ir a *Archivo-->Guardar una copia en drive*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na14Zz7LhkAV"
      },
      "source": [
        "# Parte 1: Análisis y preprocesamiento de datos\n",
        "\n",
        "Para la primera parte se debe descargar, analizar y preprocesar los datos para entrenar un clasificador basado en capas RNN, tal como se muestra en la sección *Sentiment Analysis* para los datos **sin procesar**.\n",
        "\n",
        "*   Previo al preprocesamiento, reservar un 20% (5000 críticas) de los datos de *train* para validación.\n",
        "*   Para armar la tabla del vocabulario utilizar solamente los datos de *train* después de reservar los de validación. ¿Cuáles son las 10 palabras más frecuentes en los datos de *train*?.\n",
        "*   Una vez preprocesados los datos ¿cuáles son los largos de secuencias para los primeros *batches*? ¿Es un problema a resolver que todos tengan largos distintos? ¿Por qué?.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CftLo0i0qXC2"
      },
      "source": [
        "# Parte 2: Modelo con GRU\n",
        "\n",
        "\n",
        "*   Entrenar el modelo que aparece al final de la sección *Sentiment Analysis* y previo a la sección *Masking* del capítulo 16.\n",
        "*   Mitigar el sobreajuste con las herramientas que ya conoce.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLS_jg6Tf7TG"
      },
      "source": [
        "# Parte 3: *Embedding*\n",
        "El modelo cuenta con una capa de entrada de *embedding* la cual abarca la mayoría de los parámetros entrenables. En este caso un *embedding* es un vector entrenable que representa una palabra en nuevo espacio cuyo tamaño es un hiperparámetro. La concatenación de estos vectores conforma la matriz de *embedding*, donde su cantidad de filas corresponde a la suma del tamaño del vocabulario más la cantidad reservada para los *out-of-vocabulary* (vocab_size + num_oov_buckets), y la cantidad de columnas a las dimensiones de los vectores (*embed_size*). Al igual que una matriz de pesos, ésta se inicializa de forma aleatoria, y actualiza sus valores para cada *step* de entrenamiento.\n",
        "\n",
        "*   ¿Cuál es la ventaja de utilizar una capa de *embedding*? (Ver la sección *Encoding Categorical Features Using Embeddings* del capítulo 13 del libro.)\n",
        "*   Visualizar una representación del espacio de *embedding* utilizando *Comet*.\n",
        "\n",
        "Para este último punto se recomienda seguir el siguiente ejemplo: [logging-embeddings](https://www.comet.ml/docs/user-interface/embeddings/#logging-embeddings). Tener en cuenta que el parámetro *labels* de la función tiene que ser un *array* de *strings*, por lo cual si las palabras están codificadas con *utf-8* es necesario decodificarlas: \n",
        "\n",
        "\n",
        "```\n",
        "decoder = np.vectorize(lambda x: x.decode('UTF-8'))\n",
        "words_dec = decoder(words.numpy())\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "*   Para su mejor modelo: visualizar a qué distancias se encuentran las palabras unas de otras tanto en la representación a baja dimensión como en el espacio de *embedding*. Sobre todo probar con adjetivos positivos (*wonderful*, *excellent*, etc.) y negativos (*ugly*, *boring*, etc.) comparando los resultados. ¿Qué logra observar?.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0rM5cnzh1qM"
      },
      "source": [
        "# Parte 4: *Embedding* preentrenado\n",
        "Una de las técnicas para mejorar el desempeño en este tipo de problemas es, utilizar *embeddings* ya entrenados. \n",
        "Siguiendo el ejemplo [Using pre-trained word embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/):\n",
        "\n",
        "\n",
        "*   Descargar el *embedding* preentrenado [GloVE](https://nlp.stanford.edu/projects/glove/) que aparece en la sección *Load pre-trained word embeddings*.\n",
        "*   Preparar la nueva matriz de *embedding*. ¿Cuántas palabras del conjunto de entrenamiento se encuentran en el vocabulario de GloVE? ¿Cuántas no?.\n",
        "*   Entrenar el modelo con la nueva matriz de *embedding* de manera que los valores de ésta se mantengan fijos (ver parámetro en la capa de *embedding*).\n",
        "*   Comparar con los modelos anteriores en cuanto al desempeño, la cantidad de parámetros y el tiempo de entrenamiento.\n",
        "*   Visualizar cómo es el espacio de *embedding*. ¿Qué diferencias puede observar con respecto al anterior?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chvshbcumRs7"
      },
      "source": [
        "# Parte 5: Capas Bidireccionales (opcional).\n",
        "Mirando la sección *Bidirectional RNNs* responda la siguientes preguntas:\n",
        "*   ¿Por qué puede tener sentido utilizarlas en un problema de NLP?\n",
        "*   ¿Cómo funcionan?\n",
        "*   Cambiar alguno de los modelos anteriores para que sus capas recurrentes sean bidireccionales. Compare el desempeño.\n",
        "\n"
      ]
    }
  ]
}